## 1. Introductory material

[*Home*](../README.md)

**Before you start:** You need to have watched the **first three videos** from [this playlist](https://www.youtube.com/playlist?list=PLXBJk7WTnAgWNib_2rO6EKZ0DiTRfbtSJ) on YouTube.

**About:** This is a collection of 3+1 Python notebooks serving as a warm-up. 
We start with [sampling](./1_sampling.ipynb) techniques in Python â€” sampling is 
a powerful we use regularly in estimation theory. Next, we look briefly at the
[law of the unconscious statistician (LotUS)](./2_lotus.ipynb), which allows us to determine the expectation
and variance of (nonlinear) function of a given random variable. Lastly, we study the 
concept of **[conditioning](./3_conditioning.ipynb)** which is of central importance in estimation theory.

**Contents:**

1. [Sampling](./1_sampling.ipynb)
2. [Law of the unconscious statistician (LotUS)](./2_lotus.ipynb)
3. [Conditioning](./3_conditioning.ipynb)
4. [Tutorial](./4_tutorial.ipynb)

**Where to go next:** 
1. Watch the **fourth** and **fifth** videos from [this playlist](https://www.youtube.com/playlist?list=PLXBJk7WTnAgWNib_2rO6EKZ0DiTRfbtSJ).
2. Move on to the [Kalman filter](../kalman/README.md)